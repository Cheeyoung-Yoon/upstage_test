{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1geoDYZNW4WDuw-XCSmublBXJA-_nZ6e_",
      "authorship_tag": "ABX9TyOoEwMvO3LEBoQY4nnV2GZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cheeyoung-Yoon/upstage_test/blob/main/logic_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWxd5mMvFS-U"
      },
      "outputs": [],
      "source": [
        "# parts_config.py\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "DEFAULT_LABEL_LIST: List[str] = [\n",
        "    'no_relation','org:top_members/employees','org:members','org:product','per:title',\n",
        "    'org:alternate_names','per:employee_of','org:place_of_headquarters','per:product',\n",
        "    'org:number_of_employees/members','per:children','per:place_of_residence',\n",
        "    'per:alternate_names','per:other_family','per:colleagues','per:origin',\n",
        "    'per:siblings','per:spouse','org:founded','org:political/religious_affiliation',\n",
        "    'org:member_of','per:parents','org:dissolved','per:schools_attended',\n",
        "    'per:date_of_death','per:date_of_birth','per:place_of_birth','per:place_of_death',\n",
        "    'org:founded_by','per:religion'\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    model_name: str = \"klue/roberta-base\"\n",
        "    output_dir: str = \"./runs\"\n",
        "    num_train_epochs: int = 5\n",
        "    learning_rate: float = 2e-5\n",
        "    per_device_train_batch_size: int = 16\n",
        "    per_device_eval_batch_size: int = 16\n",
        "    warmup_ratio: float = 0.05\n",
        "    weight_decay: float = 0.01\n",
        "    logging_steps: int = 500\n",
        "    save_steps: int = 500\n",
        "    eval_steps: int = 500\n",
        "    save_total_limit: int = 2\n",
        "    load_best_model_at_end: bool = True\n",
        "    seed: int = 42\n",
        "    max_length: int = 256\n",
        "    fp16: bool = True\n",
        "\n",
        "    # ÌëúÌòÑ/ÌÜ†ÌÅ∞\n",
        "    inline_markers: bool = True\n",
        "    marker_variant: str = \"typed\"   # [\"typed\",\"plain\"]\n",
        "\n",
        "    # ÏÜêÏã§/Ï†ïÍ∑úÌôî/Ïä§ÏºÄÏ§ÑÎü¨\n",
        "    label_smoothing: float = 0.1\n",
        "    lr_scheduler_type: str = \"cosine\"\n",
        "    use_class_weight: bool = False\n",
        "    use_cb_loss: bool = False\n",
        "    use_focal: bool = False\n",
        "    focal_gamma: float = 2.0\n",
        "    rdrop_alpha: float = 0.0\n",
        "\n",
        "    # ÏµúÏ†ÅÌôî\n",
        "    use_llrd: bool = False\n",
        "    llrd_decay: float = 0.95\n",
        "\n",
        "    # Íµ¨Ï°∞/Ìä∏Î¶≠\n",
        "    use_marker_head: bool = True\n",
        "    use_erpe: bool = False\n",
        "    erpe_dim: int = 32\n",
        "    use_fgm: bool = False\n",
        "    fgm_eps: float = 1e-3\n",
        "\n",
        "    # ÌïòÎìú ÎÑ§Í±∞Ìã∞Î∏å\n",
        "    use_hardneg: bool = False\n",
        "    hardneg_tau: float = 0.55\n",
        "    hardneg_boost: float = 2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_plus.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import ast\n",
        "from typing import Optional\n",
        "\n",
        "class RE_Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\" tokenized dict + labels (+ optional weights) \"\"\"\n",
        "    def __init__(self, pair_dataset: dict, labels, weights: Optional[np.ndarray]=None):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.labels = labels\n",
        "        self.weights = weights if weights is not None else np.ones(len(labels), dtype=np.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx].clone().detach() for k, v in self.pair_dataset.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def preprocessing_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    def pick_word(s):\n",
        "        try:\n",
        "            d = ast.literal_eval(s) if isinstance(s, str) else s\n",
        "            return d.get(\"word\"), d.get(\"type\")\n",
        "        except Exception:\n",
        "            return None, None\n",
        "    subj_words, obj_words = [], []\n",
        "    for s, o in zip(df[\"subject_entity\"], df[\"object_entity\"]):\n",
        "        sw, _ = pick_word(s); ow, _ = pick_word(o)\n",
        "        subj_words.append(sw or \"<SUBJ>\"); obj_words.append(ow or \"<OBJ>\")\n",
        "    out = pd.DataFrame({\n",
        "        \"id\": df[\"id\"],\n",
        "        \"sentence\": df[\"sentence\"],\n",
        "        \"subject_entity\": df[\"subject_entity\"],\n",
        "        \"object_entity\": df[\"object_entity\"],\n",
        "        \"subj_word\": subj_words, \"obj_word\": obj_words,\n",
        "        \"label\": df[\"label\"],\n",
        "    })\n",
        "    return out\n",
        "\n",
        "\n",
        "def load_data(csv_path: str) -> pd.DataFrame:\n",
        "    # Accept a single path or (path,) tuple/list and normalize to string\n",
        "    if isinstance(csv_path, (list, tuple)):\n",
        "        csv_path = csv_path[0]\n",
        "    return preprocessing_dataset(pd.read_csv(csv_path))\n",
        "\n",
        "\n",
        "def _inline_mark(sentence: str, s_word: str, o_word: str, s_type: Optional[str], o_type: Optional[str], use_type: bool, use_unk: bool):\n",
        "    # Ï≤´ Îì±Ïû•Îßå ÏπòÌôò (Îã®Ïñ¥ Í≤ΩÍ≥Ñ Í≥†Î†§)\n",
        "    def repl_first(text, pat, repl):\n",
        "        m = re.search(rf'(?<!\\w){re.escape(pat)}(?!\\w)', text)\n",
        "        if not m: return text\n",
        "        return text[:m.start()] + repl + text[m.start():m.end()].replace(pat,\"\") + text[m.end():]\n",
        "\n",
        "    if use_type:\n",
        "        s_type = s_type or (\"UNK\" if use_unk else None)\n",
        "        o_type = o_type or (\"UNK\" if use_unk else None)\n",
        "        if s_type and o_type:\n",
        "            s_tag = f\"[E1-{s_type}]{s_word}[/E1]\"\n",
        "            o_tag = f\"[E2-{o_type}]{o_word}[/E2]\"\n",
        "        else:\n",
        "            s_tag = f\"[E1]{s_word}[/E1]\"; o_tag = f\"[E2]{o_word}[/E2]\"\n",
        "    else:\n",
        "        s_tag = f\"[E1]{s_word}[/E1]\"; o_tag = f\"[E2]{o_word}[/E2]\"\n",
        "\n",
        "    tmp = repl_first(sentence, s_word, s_tag)\n",
        "    tmp = repl_first(tmp, o_word, o_tag)\n",
        "    return tmp\n",
        "def tokenized_dataset(df, tokenizer, *,\n",
        "                      inline_markers=True, marker_variant=\"typed\", use_unk=True, max_len=256, use_erpe=False):\n",
        "    enc_inputs = []\n",
        "    for _, r in df.iterrows():\n",
        "        s = ast.literal_eval(r[\"subject_entity\"]) if isinstance(r[\"subject_entity\"], str) else r[\"subject_entity\"]\n",
        "        o = ast.literal_eval(r[\"object_entity\"]) if isinstance(r[\"object_entity\"], str) else r[\"object_entity\"]\n",
        "\n",
        "        if inline_markers:\n",
        "            text = _inline_mark(\n",
        "                r[\"sentence\"],\n",
        "                s.get(\"word\") if s else r[\"subj_word\"],\n",
        "                o.get(\"word\") if o else r[\"obj_word\"],\n",
        "                (s or {}).get(\"type\"), (o or {}).get(\"type\"),\n",
        "                use_type=(marker_variant == \"typed\"), use_unk=use_unk\n",
        "            )\n",
        "            enc_inputs.append(text)\n",
        "        else:\n",
        "            # Îëê Í∞úÏùò separate sequenceÎ°ú Íµ¨ÏÑ±\n",
        "            if marker_variant == \"typed\":\n",
        "                span = f\"[E1-{(s or {}).get('type','UNK')}]{(s or {}).get('word','<SUBJ>')}[/E1] \" \\\n",
        "                       f\"[E2-{(o or {}).get('type','UNK')}]{(o or {}).get('word','<OBJ>')}[/E2]\"\n",
        "            else:\n",
        "                span = f\"[E1]{(s or {}).get('word','<SUBJ>')}[/E1] [E2]{(o or {}).get('word','<OBJ>')}[/E2]\"\n",
        "            enc_inputs.append((span, r[\"sentence\"]))\n",
        "\n",
        "    # üîπ Î∞©Ïñ¥ ÏΩîÎìú: inline_markers Ïó¨Î∂ÄÏóê Îî∞Îùº tokenizer ÏûÖÎ†• Î∞©Ïãù Í≤∞Ï†ï\n",
        "    if inline_markers:\n",
        "        if enc_inputs and isinstance(enc_inputs[0], tuple):\n",
        "            raise ValueError(\"[tokenized_dataset] inline_markers=TrueÏù∏Îç∞ tuple ÌòïÏãùÏù¥ Í∞êÏßÄÎê®.\")\n",
        "        enc = tokenizer(\n",
        "            enc_inputs,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "    else:\n",
        "        if not enc_inputs or not isinstance(enc_inputs[0], tuple):\n",
        "            raise ValueError(\"[tokenized_dataset] inline_markers=FalseÏù∏Îç∞ tuple ÌòïÏãùÏù¥ ÏïÑÎãò.\")\n",
        "        a, b = zip(*enc_inputs)\n",
        "        enc = tokenizer(\n",
        "            list(a), list(b),\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "    # ERPE Ï∂îÍ∞Ä Ï≤òÎ¶¨\n",
        "    if use_erpe:\n",
        "        e1_id = tokenizer.convert_tokens_to_ids(\"[E1]\")\n",
        "        e2_id = tokenizer.convert_tokens_to_ids(\"[E2]\")\n",
        "\n",
        "        def relpos(ids, mark_id, clip=128):\n",
        "            pos = (ids == mark_id).nonzero(as_tuple=True)[0]\n",
        "            m = int(pos[0]) if len(pos) else 0\n",
        "            ar = torch.arange(ids.size(0)) - m\n",
        "            ar.clamp_(-clip, clip).add_(clip)\n",
        "            return ar\n",
        "\n",
        "        input_ids = enc[\"input_ids\"]\n",
        "        enc[\"e1_relpos\"] = torch.stack([relpos(row, e1_id) for row in input_ids])\n",
        "        enc[\"e2_relpos\"] = torch.stack([relpos(row, e2_id) for row in input_ids])\n",
        "\n",
        "    enc.pop(\"token_type_ids\", None)\n",
        "    return enc\n",
        "\n"
      ],
      "metadata": {
        "id": "a4-03JvoFUEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_builders.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "class REMarkerHead(nn.Module):\n",
        "    def __init__(self, base_model, hidden_size, num_labels, use_cls=False, dropout=0.1, e1_id=None, e2_id=None):\n",
        "        super().__init__()\n",
        "        self.backbone = base_model\n",
        "        self.use_cls = use_cls\n",
        "        self.e1_id = e1_id\n",
        "        self.e2_id = e2_id\n",
        "        in_dim = hidden_size * (3 if use_cls else 2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(in_dim, num_labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pick_first(mask, H):\n",
        "        idx = mask.float().argmax(dim=1)\n",
        "        return H[torch.arange(H.size(0), device=H.device), idx]\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, **kw):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        H = out.last_hidden_state\n",
        "        e1_mask = (input_ids == self.e1_id); e2_mask = (input_ids == self.e2_id)\n",
        "        h1 = self._pick_first(e1_mask, H); h2 = self._pick_first(e2_mask, H)\n",
        "        feats = [h1, h2]\n",
        "        if self.use_cls: feats.append(H[:,0,:])\n",
        "        x = self.dropout(torch.cat(feats, dim=-1))\n",
        "        logits = self.classifier(x)\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "class REMarkerHeadERPE(REMarkerHead):\n",
        "    def __init__(self, base_model, hidden_size, num_labels, erpe_dim=32, rel_vocab=257, **kw):\n",
        "        super().__init__(base_model, hidden_size, num_labels, **kw)\n",
        "        self.e1_pos_emb = nn.Embedding(rel_vocab, erpe_dim)\n",
        "        self.e2_pos_emb = nn.Embedding(rel_vocab, erpe_dim)\n",
        "        self.proj = nn.Linear(hidden_size + 2*erpe_dim, hidden_size)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, e1_relpos=None, e2_relpos=None, **kw):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        H = out.last_hidden_state\n",
        "        P = torch.cat([self.e1_pos_emb(e1_relpos), self.e2_pos_emb(e2_relpos)], dim=-1)\n",
        "        H = self.proj(torch.cat([H, P], dim=-1))\n",
        "        e1_mask = (input_ids == self.e1_id); e2_mask = (input_ids == self.e2_id)\n",
        "        h1 = self._pick_first(e1_mask, H); h2 = self._pick_first(e2_mask, H)\n",
        "        x = self.dropout(torch.cat([h1, h2], dim=-1))\n",
        "        logits = self.classifier(x)\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "def build_model(model_name: str, num_labels: int, tokenizer, *, use_marker_head=True, use_erpe=False, erpe_dim=32):\n",
        "    cfg = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
        "    base = AutoModel.from_pretrained(model_name, config=cfg)\n",
        "    # special tokens resizeÎäî Î∞îÍπ•ÏóêÏÑú Ïù¥ÎØ∏ Ï≤òÎ¶¨ÌñàÎã§Í≥† Í∞ÄÏ†ï\n",
        "\n",
        "    if not use_marker_head:\n",
        "        from transformers import AutoModelForSequenceClassification\n",
        "        return AutoModelForSequenceClassification.from_pretrained(model_name, config=cfg)\n",
        "\n",
        "    e1_id = tokenizer.convert_tokens_to_ids(\"[E1]\")\n",
        "    e2_id = tokenizer.convert_tokens_to_ids(\"[E2]\")\n",
        "    if use_erpe:\n",
        "        return REMarkerHeadERPE(base_model=base, hidden_size=cfg.hidden_size, num_labels=num_labels,\n",
        "                                erpe_dim=erpe_dim, e1_id=e1_id, e2_id=e2_id)\n",
        "    else:\n",
        "        return REMarkerHead(base_model=base, hidden_size=cfg.hidden_size, num_labels=num_labels,\n",
        "                            use_cls=False, e1_id=e1_id, e2_id=e2_id)\n"
      ],
      "metadata": {
        "id": "REkkJKR5FXNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer_plus.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from transformers import Trainer\n",
        "\n",
        "def build_llrd_param_groups(model, base_lr=2e-5, lr_decay=0.95, wd=0.01):\n",
        "    groups = []\n",
        "\n",
        "    def collect(names):\n",
        "        return [p for n,p in model.named_parameters() if any(k in n for k in names) and p.requires_grad]\n",
        "\n",
        "    # üîß ÏûÑÎ≤†Îî©ÏùÑ word / position / layer-normÏúºÎ°ú Î∂ÑÎ¶¨\n",
        "    emb_word = collect([\"embeddings.word_embeddings\"])\n",
        "    emb_pos  = collect([\"embeddings.position_embeddings\"])\n",
        "    emb_ln   = collect([\"embeddings.LayerNorm\"])\n",
        "\n",
        "    if emb_word: groups.append({\"params\": emb_word, \"lr\": base_lr*(lr_decay**12), \"weight_decay\": wd})\n",
        "    if emb_pos:  groups.append({\"params\": emb_pos,  \"lr\": base_lr*(lr_decay**12), \"weight_decay\": 0.0})  # Î≥¥ÌÜµ decay=0\n",
        "    if emb_ln:   groups.append({\"params\": emb_ln,   \"lr\": base_lr*(lr_decay**12), \"weight_decay\": 0.0})\n",
        "\n",
        "    for i in range(12):\n",
        "        groups.append({\"params\": collect([f\"encoder.layer.{i}\"]), \"lr\": base_lr*(lr_decay**(11-i)), \"weight_decay\": wd})\n",
        "\n",
        "    # pooler / classifier\n",
        "    groups.append({\"params\": collect([\"pooler\", \"classifier\"]), \"lr\": base_lr, \"weight_decay\": wd})\n",
        "    return groups\n",
        "\n",
        "class TrainerPlus(Trainer):\n",
        "    def __init__(self, *args,\n",
        "                 class_weights=None,\n",
        "                 use_focal=False, focal_gamma=2.0,\n",
        "                 rdrop_alpha=0.0,\n",
        "                 use_llrd=False, llrd_decay=0.95,\n",
        "                 optimizer_betas=(0.9, 0.999),\n",
        "                 wd=0.01,\n",
        "                 use_fgm=False, fgm_eps=1e-3,\n",
        "                 **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "        self.use_focal = use_focal\n",
        "        self.focal_gamma = focal_gamma\n",
        "        self.rdrop_alpha = rdrop_alpha\n",
        "        self.use_llrd = use_llrd\n",
        "        self.llrd_decay = llrd_decay\n",
        "        self.optimizer_betas = optimizer_betas\n",
        "        self._wd = wd\n",
        "        self.use_fgm = use_fgm\n",
        "        self.fgm_eps = fgm_eps\n",
        "        self._fgm_backup = {}\n",
        "\n",
        "    # ---- losses ----\n",
        "    def _ce(self, logits, labels):\n",
        "        return F.cross_entropy(logits, labels, weight=self.class_weights)\n",
        "\n",
        "    def _focal(self, logits, labels):\n",
        "        ce = F.cross_entropy(logits, labels, reduction=\"none\", weight=self.class_weights)\n",
        "        p = logits.softmax(dim=-1)[torch.arange(len(labels), device=logits.device), labels]\n",
        "        return ((1 - p) ** self.focal_gamma * ce).mean()\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        model,\n",
        "        inputs,\n",
        "        return_outputs: bool = False,\n",
        "        num_items_in_batch=None,   # ‚úÖ ÏµúÏã† HFÍ∞Ä ÎÑòÍ∏∞Îäî Ïù∏Ïûê ÏàòÏö©\n",
        "        **kwargs,                  # ‚úÖ ÏïûÏúºÎ°úÏùò ÌôïÏû• ÎåÄÎπÑ\n",
        "    ):\n",
        "        labels = inputs[\"labels\"]\n",
        "        out1 = model(**inputs)\n",
        "        logits1 = out1[\"logits\"]\n",
        "\n",
        "        base = self._focal(logits1, labels) if self.use_focal else self._ce(logits1, labels)\n",
        "\n",
        "        if self.rdrop_alpha > 0 and model.training:\n",
        "            out2 = model(**inputs)\n",
        "            logits2 = out2[\"logits\"]\n",
        "            base2 = self._focal(logits2, labels) if self.use_focal else self._ce(logits2, labels)\n",
        "            base = 0.5 * (base + base2)\n",
        "            p1 = logits1.log_softmax(dim=-1); p2 = logits2.log_softmax(dim=-1)\n",
        "            kl = F.kl_div(p1, p2.exp(), reduction=\"batchmean\") + F.kl_div(p2, p1.exp(), reduction=\"batchmean\")\n",
        "            loss = base + 0.5 * self.rdrop_alpha * kl\n",
        "            return (loss, out1) if return_outputs else loss\n",
        "\n",
        "        return (base, out1) if return_outputs else base\n",
        "\n",
        "    # ---- FGM by overriding training_step (proper sequence) ----\n",
        "    def _fgm_attack(self, emb_name=\"embeddings.word_embeddings\"):\n",
        "        for n, p in self.model.named_parameters():\n",
        "            if p.requires_grad and emb_name in n and p.grad is not None:\n",
        "                self._fgm_backup[n] = p.data.clone()\n",
        "                g = p.grad / (p.grad.norm() + 1e-12)\n",
        "                p.data.add_(self.fgm_eps * g)\n",
        "\n",
        "    def _fgm_restore(self):\n",
        "        for n, p in self.model.named_parameters():\n",
        "            if n in self._fgm_backup:\n",
        "                p.data = self._fgm_backup[n]\n",
        "        self._fgm_backup.clear()\n",
        "\n",
        "\n",
        "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
        "        # 1) HF Í∏∞Î≥∏ training_step Î®ºÏ†Ä Ìò∏Ï∂ú ‚Üí AMP/Scaler/accumulation Î™®Îëê ÏïàÏ†Ñ Ï≤òÎ¶¨\n",
        "        base_loss_detached = super().training_step(model, inputs, num_items_in_batch)\n",
        "\n",
        "        # 2) FGM ÏûàÏúºÎ©¥ Îëê Î≤àÏß∏ forward/backward (Ïä§ÏºÄÏùºÎü¨ Í∑úÏπô Í∑∏ÎåÄÎ°ú ÎßûÏ∂∞Ï§å)\n",
        "        if self.use_fgm:\n",
        "            self._fgm_attack()\n",
        "            with self.autocast_smart_context_manager():\n",
        "                adv_loss = self.compute_loss(model, self._prepare_inputs(inputs))\n",
        "            if self.args.n_gpu > 1:\n",
        "                adv_loss = adv_loss.mean()\n",
        "            adv_loss = adv_loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "            # HFÍ∞Ä ÏÑ§Ï†ïÌïú Ïä§ÏºÄÏùºÎü¨ ÌîåÎûòÍ∑∏Î•º Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
        "            use_scaler = getattr(self, \"do_grad_scaling\", False) and getattr(self, \"scaler\", None) is not None\n",
        "            if use_scaler:\n",
        "                self.scaler.scale(adv_loss).backward()\n",
        "            else:\n",
        "                adv_loss.backward()\n",
        "            self._fgm_restore()\n",
        "\n",
        "        return base_loss_detached\n",
        "\n",
        "\n",
        "    # ---- optimizer with LLRD ----\n",
        "    def create_optimizer(self):\n",
        "        if self.optimizer is not None:\n",
        "            return self.optimizer\n",
        "        lr = self.args.learning_rate; wd = self._wd; betas = self.optimizer_betas\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
        "        if self.use_llrd:\n",
        "            base_groups = build_llrd_param_groups(self.model, base_lr=lr, lr_decay=self.llrd_decay, wd=wd)\n",
        "            groups = []\n",
        "            for g in base_groups:\n",
        "                dec, nde = [], []\n",
        "                for n, p in self.model.named_parameters():\n",
        "                    if p not in g[\"params\"] or not p.requires_grad: continue\n",
        "                    (nde if any(nd in n for nd in no_decay) else dec).append(p)\n",
        "                if dec: groups.append({\"params\": dec, \"lr\": g[\"lr\"], \"weight_decay\": wd})\n",
        "                if nde: groups.append({\"params\": nde, \"lr\": g[\"lr\"], \"weight_decay\": 0.0})\n",
        "            param_groups = groups\n",
        "        else:\n",
        "            dec, nde = [], []\n",
        "            for n,p in self.model.named_parameters():\n",
        "                if not p.requires_grad: continue\n",
        "                (nde if any(nd in n for nd in no_decay) else dec).append(p)\n",
        "            param_groups = [\n",
        "                {\"params\": dec, \"weight_decay\": wd, \"lr\": lr},\n",
        "                {\"params\": nde, \"weight_decay\": 0.0, \"lr\": lr},\n",
        "            ]\n",
        "        self.optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=betas)\n",
        "        return self.optimizer\n",
        "\n",
        "    # ---- weighted sampler (for hard-neg callback) ----\n",
        "    def get_train_dataloader(self):\n",
        "        if hasattr(self.train_dataset, \"weights\") and self.train_dataset.weights is not None:\n",
        "            sampler = WeightedRandomSampler(self.train_dataset.weights, num_samples=len(self.train_dataset), replacement=True)\n",
        "            return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size,\n",
        "                              sampler=sampler, collate_fn=self.data_collator)\n",
        "        return super().get_train_dataloader()\n"
      ],
      "metadata": {
        "id": "icN9kyWKFYkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hardneg_callback.py\n",
        "import torch, numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "class HardNegSampler(TrainerCallback):\n",
        "    def __init__(self, no_rel_id=0, tau=0.55, boost=2.0):\n",
        "        self.no_rel_id = no_rel_id\n",
        "        self.tau = tau\n",
        "        self.boost = boost\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kw):\n",
        "        tr = kw[\"trainer\"]\n",
        "        ds = tr.train_dataset\n",
        "        if not hasattr(ds, \"weights\"):\n",
        "            ds.weights = np.ones(len(ds), dtype=np.float32)\n",
        "\n",
        "        dl = DataLoader(ds, batch_size=args.per_device_eval_batch_size)\n",
        "        probs_all, labels_all = [], []\n",
        "        tr.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in dl:\n",
        "                batch = {k: v.to(tr.model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "                logits = tr.model(**batch)[\"logits\"]\n",
        "                probs_all.append(logits.softmax(-1).cpu())\n",
        "                labels_all.append(batch[\"labels\"].cpu())\n",
        "        probs = torch.cat(probs_all).numpy()\n",
        "        labels = torch.cat(labels_all).numpy()\n",
        "        p_nr = probs[:, self.no_rel_id]\n",
        "        hard = (labels == self.no_rel_id) & (p_nr < self.tau)\n",
        "\n",
        "        w = ds.weights.astype(np.float32)\n",
        "        w[hard] *= self.boost\n",
        "        ds.weights = w\n",
        "        tr.train_dataloader = None  # Ïû¨ÏÉùÏÑ± Ìä∏Î¶¨Í±∞\n"
      ],
      "metadata": {
        "id": "MR9Ks7zGFZy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_re.py\n",
        "import os, numpy as np, torch\n",
        "from typing import List, Optional\n",
        "from dataclasses import replace\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, EarlyStoppingCallback\n",
        "import sklearn\n",
        "\n",
        "# from parts_config import TrainConfig, DEFAULT_LABEL_LIST\n",
        "# from data_plus import RE_Dataset, load_data, tokenized_dataset\n",
        "# from model_builders import build_model\n",
        "# from trainer_plus import TrainerPlus\n",
        "\n",
        "def train_re(\n",
        "    train_csv: str,\n",
        "    dev_csv: Optional[str],\n",
        "    label_list: List[str] = DEFAULT_LABEL_LIST,\n",
        "    cfg: TrainConfig = TrainConfig(),\n",
        "    save_best_to: str = \"./best_model\",\n",
        "    callbacks=None,\n",
        "):\n",
        "    torch.manual_seed(cfg.seed); np.random.seed(cfg.seed)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "    special_tokens = [\"[E1]\",\"[/E1]\",\"[E2]\",\"[/E2]\"]\n",
        "    if cfg.marker_variant == \"typed\":\n",
        "        special_tokens += [\"[E1-PER]\",\"[E2-PER]\",\"[E1-ORG]\",\"[E2-ORG]\",\"[E1-LOC]\",\"[E2-LOC]\",\"[E1-UNK]\",\"[E2-UNK]\"]\n",
        "    added = tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
        "\n",
        "    model = build_model(\n",
        "        cfg.model_name,\n",
        "        num_labels=len(label_list),\n",
        "        tokenizer=tokenizer,\n",
        "        use_marker_head=cfg.use_marker_head,\n",
        "        use_erpe=cfg.use_erpe,\n",
        "        erpe_dim=cfg.erpe_dim\n",
        "    )\n",
        "\n",
        "    if added > 0:\n",
        "        if hasattr(model, \"resize_token_embeddings\"):\n",
        "            model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
        "        elif hasattr(model, \"backbone\") and hasattr(model.backbone, \"resize_token_embeddings\"):\n",
        "            model.backbone.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    full_df = load_data(train_csv)\n",
        "    if dev_csv:\n",
        "        train_df, dev_df = full_df, load_data(dev_csv)\n",
        "    else:\n",
        "        train_df, dev_df = train_test_split(full_df, test_size=0.1, random_state=cfg.seed, stratify=full_df[\"label\"])\n",
        "\n",
        "    label_map = {v: i for i, v in enumerate(label_list)}\n",
        "    y_tr = [label_map[v] for v in train_df[\"label\"].values]\n",
        "    y_dv = [label_map[v] for v in dev_df[\"label\"].values]\n",
        "\n",
        "    tok_tr = tokenized_dataset(train_df, tokenizer,\n",
        "                               inline_markers=cfg.inline_markers, marker_variant=cfg.marker_variant,\n",
        "                               max_len=cfg.max_length, use_erpe=cfg.use_erpe)\n",
        "    tok_dv = tokenized_dataset(dev_df, tokenizer,\n",
        "                               inline_markers=cfg.inline_markers, marker_variant=cfg.marker_variant,\n",
        "                               max_len=cfg.max_length, use_erpe=cfg.use_erpe)\n",
        "\n",
        "    ds_tr = RE_Dataset(tok_tr, y_tr)\n",
        "    ds_dv = RE_Dataset(tok_dv, y_dv)\n",
        "\n",
        "    # class weights\n",
        "    class_weights = None\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    if cfg.use_cb_loss:\n",
        "        beta = 0.999\n",
        "        counts = np.bincount(y_tr, minlength=len(label_list))\n",
        "        eff_num = 1.0 - np.power(beta, counts)\n",
        "        cbw = (1.0 - beta) / np.clip(eff_num, 1e-6, None)\n",
        "        cbw = cbw / cbw.mean()\n",
        "        class_weights = torch.tensor(cbw, dtype=torch.float32, device=device)\n",
        "    elif cfg.use_class_weight:\n",
        "        counts = np.bincount(y_tr, minlength=len(label_list))\n",
        "        inv = 1.0 / np.clip(counts, 1, None)\n",
        "        w = inv / inv.mean()\n",
        "        class_weights = torch.tensor(w, dtype=torch.float32, device=device)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=cfg.output_dir,\n",
        "        save_total_limit=cfg.save_total_limit,\n",
        "\n",
        "        # ‚úÖ ÌèâÍ∞Ä/Ï†ÄÏû• ÌÉÄÏù¥Î∞çÏùÑ 'steps'Î°ú ÌÜµÏùºÌïòÍ≥†, Ïä§ÌÖùÎèÑ ÎèôÏùºÌïòÍ≤å\n",
        "        eval_strategy=\"steps\",      # <-- eval_strategy(X)\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=cfg.eval_steps,\n",
        "        save_steps=cfg.eval_steps,\n",
        "\n",
        "        num_train_epochs=cfg.num_train_epochs,\n",
        "        learning_rate=cfg.learning_rate,\n",
        "        per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=cfg.per_device_eval_batch_size,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=cfg.logging_steps,\n",
        "\n",
        "        load_best_model_at_end=cfg.load_best_model_at_end,\n",
        "        metric_for_best_model= \"micro_f1\",   # <-- 'eval_' Î∂ôÏù¥Î©¥ Ïïà Îê®\n",
        "        greater_is_better=True,\n",
        "\n",
        "        seed=cfg.seed,\n",
        "        fp16=cfg.fp16 and torch.cuda.is_available(),\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_pin_memory=torch.cuda.is_available(),\n",
        "        report_to=\"none\",\n",
        "        label_smoothing_factor=cfg.label_smoothing,\n",
        "        lr_scheduler_type=cfg.lr_scheduler_type,\n",
        "        warmup_ratio=cfg.warmup_ratio,\n",
        "    )\n",
        "    def micro_f1_wo_no_relation(preds, labels, label_list, no_rel=\"no_relation\"):\n",
        "        no_rel_idx = label_list.index(no_rel)\n",
        "        use_labels = [i for i in range(len(label_list)) if i != no_rel_idx]\n",
        "        return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=use_labels) * 100.0\n",
        "\n",
        "    def auprc_all(probs, labels, num_labels):\n",
        "        labels_oh = np.eye(num_labels)[labels]\n",
        "        score = []\n",
        "        for c in range(num_labels):\n",
        "            t = labels_oh[:, c]; p = probs[:, c]\n",
        "            prec, rec, _ = sklearn.metrics.precision_recall_curve(t, p)\n",
        "            score.append(sklearn.metrics.auc(rec, prec))\n",
        "        return float(np.mean(score) * 100.0)\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        if hasattr(eval_pred, \"predictions\"):\n",
        "            logits = eval_pred.predictions\n",
        "            labels = eval_pred.label_ids\n",
        "        else:\n",
        "            logits, labels = eval_pred\n",
        "\n",
        "        import numpy as np, torch\n",
        "        if isinstance(logits, tuple): logits = logits[0]\n",
        "        if isinstance(logits, torch.Tensor): logits = logits.detach().cpu().numpy()\n",
        "        if isinstance(labels, torch.Tensor): labels = labels.detach().cpu().numpy()\n",
        "\n",
        "        preds = logits.argmax(-1)\n",
        "        probs = (torch.tensor(logits).softmax(-1)).numpy()\n",
        "\n",
        "        from sklearn.metrics import f1_score, accuracy_score, precision_recall_curve, auc\n",
        "        no_rel_idx = DEFAULT_LABEL_LIST.index(\"no_relation\")\n",
        "        use_labels = [i for i in range(len(DEFAULT_LABEL_LIST)) if i != no_rel_idx]\n",
        "        micro_f1 = f1_score(labels, preds, average=\"micro\", labels=use_labels) * 100.0\n",
        "\n",
        "        labels_oh = np.eye(logits.shape[1])[labels]\n",
        "        auprc = np.mean([\n",
        "            auc(*precision_recall_curve(labels_oh[:, c], probs[:, c])[1::-1])\n",
        "            for c in range(logits.shape[1])\n",
        "        ]) * 100.0\n",
        "\n",
        "        return {\n",
        "            \"micro f1 score\": micro_f1,      # ‚úÖ Ïù¥ ÌÇ§ Ïù¥Î¶ÑÍ≥º Ï†ïÌôïÌûà ÏùºÏπòÌï¥Ïïº Ìï®\n",
        "            \"auprc\": auprc,\n",
        "            \"accuracy\": accuracy_score(labels, preds) * 100.0,\n",
        "        }\n",
        "\n",
        "    default_cbs = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "    use_callbacks = default_cbs + (callbacks or [])\n",
        "\n",
        "    trainer = TrainerPlus(\n",
        "        model=model, args=args,\n",
        "        train_dataset=ds_tr, eval_dataset=ds_dv,\n",
        "        compute_metrics=compute_metrics, processing_class=tokenizer,\n",
        "        class_weights=class_weights,\n",
        "        use_focal=cfg.use_focal, focal_gamma=cfg.focal_gamma,\n",
        "        rdrop_alpha=cfg.rdrop_alpha,\n",
        "        use_llrd=cfg.use_llrd, llrd_decay=cfg.llrd_decay,\n",
        "        wd=cfg.weight_decay,\n",
        "        use_fgm=cfg.use_fgm, fgm_eps=cfg.fgm_eps,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=cfg.es_patience)]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    os.makedirs(save_best_to, exist_ok=True)\n",
        "    trainer.save_model(save_best_to)\n",
        "    if trainer.tokenizer: trainer.tokenizer.save_pretrained(save_best_to)\n",
        "    return trainer\n",
        "\n"
      ],
      "metadata": {
        "id": "oW1svTMnFjyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grid_plus.py\n",
        "import os, time, gc, shutil, inspect\n",
        "from typing import Optional, List, Dict, Any\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import TrainerCallback, EarlyStoppingCallback\n",
        "\n",
        "# from parts_config import TrainConfig, DEFAULT_LABEL_LIST\n",
        "# from train_re import train_re\n",
        "# from hardneg_callback import HardNegSampler\n",
        "\n",
        "class ConsoleLogger(TrainerCallback):\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        print(f\"[train] start ‚Üí out={args.output_dir} | lr={args.learning_rate} | bsz={args.per_device_train_batch_size} | epochs={args.num_train_epochs}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if not state.is_world_process_zero or not logs: return\n",
        "        keys = [\"loss\",\"learning_rate\",\"epoch\"]\n",
        "        msg = \" | \".join([f\"{k}={logs[k]:.5f}\" for k in keys if k in logs])\n",
        "        for k in [\"eval_micro f1 score\",\"eval_auprc\",\"eval_accuracy\"]:\n",
        "            if k in logs: msg += f\" | {k}={logs[k]:.3f}\"\n",
        "        print(f\"[step {state.global_step}] {msg}\")\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        print(f\"[train] end. best={state.best_model_checkpoint}\")\n",
        "\n",
        "def run_grid_plus(\n",
        "    train_csv: str,\n",
        "    dev_csv: Optional[str],\n",
        "    models: List[str],\n",
        "    hp_space: Dict[str, list],\n",
        "    base_out: str = \"./grid_runs_plus\",\n",
        "    seed_list: List[int] = (42,),\n",
        "    label_list: List[str] = None,\n",
        "    extra_callbacks: Optional[List[TrainerCallback]] = None,\n",
        ") -> pd.DataFrame:\n",
        "    if label_list is None:\n",
        "        label_list = DEFAULT_LABEL_LIST\n",
        "    os.makedirs(base_out, exist_ok=True)\n",
        "\n",
        "    keys, values = zip(*hp_space.items())\n",
        "    combos = list(product(*values))\n",
        "\n",
        "    results = []\n",
        "    total = len(models) * len(seed_list) * len(combos)\n",
        "    idx = 0\n",
        "\n",
        "    for model_name in models:\n",
        "        for seed in seed_list:\n",
        "            for vals in combos:\n",
        "                idx += 1\n",
        "                opt = dict(zip(keys, vals))\n",
        "\n",
        "                run_name = (\n",
        "                    f\"{model_name.replace('/','_')}\"\n",
        "                    f\"_lr{opt['lr']}_ep{opt['epochs']}_bs{opt['train_bsz']}\"\n",
        "                    f\"_ml{opt['max_len']}_mk{opt['marker_variant']}\"\n",
        "                    f\"_inline{int(opt['inline_markers'])}\"\n",
        "                    f\"_mh{int(opt['use_marker_head'])}_erpe{int(opt['use_erpe'])}_ed{opt['erpe_dim']}\"\n",
        "                    f\"_fgm{int(opt['use_fgm'])}\"\n",
        "                    f\"_cb{int(opt['use_cb_loss'])}_cw{int(opt['use_class_weight'])}\"\n",
        "                    f\"_focal{int(opt['use_focal'])}_rd{opt['rdrop_alpha']}_llrd{int(opt['use_llrd'])}\"\n",
        "                    f\"_hn{int(opt['use_hardneg'])}_tau{opt['hardneg_tau']}_boost{opt['hardneg_boost']}\"\n",
        "                    f\"_seed{seed}\"\n",
        "                )\n",
        "                out_dir = os.path.join(base_out, run_name)\n",
        "                best_dir = os.path.join(out_dir, \"best\")\n",
        "\n",
        "                if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                t0 = time.time()\n",
        "                row = {\"run\": run_name, \"model\": model_name, \"seed\": seed, **opt,\n",
        "                       \"output_dir\": out_dir, \"best_dir\": best_dir,\n",
        "                       \"micro_f1\": None, \"auprc\": None, \"accuracy\": None,\n",
        "                       \"micro_f1_tta\": None, \"best_ckpt\": None, \"seconds\": None, \"error\": None}\n",
        "\n",
        "                try:\n",
        "                    cfg = TrainConfig(\n",
        "                        model_name=model_name,\n",
        "                        output_dir=out_dir,\n",
        "                        num_train_epochs=opt[\"epochs\"],\n",
        "                        learning_rate=opt[\"lr\"],\n",
        "                        per_device_train_batch_size=opt[\"train_bsz\"],\n",
        "                        per_device_eval_batch_size=opt[\"train_bsz\"],\n",
        "                        warmup_ratio=opt[\"warmup_ratio\"],\n",
        "                        weight_decay=0.01,\n",
        "                        logging_steps=1000, save_steps=1000, eval_steps=1000,\n",
        "                        save_total_limit=2, load_best_model_at_end=True,\n",
        "                        seed=seed, max_length=opt[\"max_len\"], fp16=torch.cuda.is_available(),\n",
        "                        inline_markers=opt[\"inline_markers\"],\n",
        "                        marker_variant=opt[\"marker_variant\"],\n",
        "                        label_smoothing=opt[\"label_smoothing\"],\n",
        "                        lr_scheduler_type=opt[\"scheduler\"],\n",
        "                        use_class_weight=opt[\"use_class_weight\"],\n",
        "                        use_cb_loss=opt[\"use_cb_loss\"],\n",
        "                        use_focal=opt[\"use_focal\"], focal_gamma=opt[\"focal_gamma\"],\n",
        "                        rdrop_alpha=opt[\"rdrop_alpha\"],\n",
        "                        use_llrd=opt[\"use_llrd\"], llrd_decay=opt[\"llrd_decay\"],\n",
        "                        use_marker_head=opt[\"use_marker_head\"],\n",
        "                        use_erpe=opt[\"use_erpe\"], erpe_dim=opt[\"erpe_dim\"],\n",
        "                        use_fgm=opt[\"use_fgm\"], fgm_eps=opt[\"fgm_eps\"],\n",
        "                        use_hardneg=opt[\"use_hardneg\"],\n",
        "                        hardneg_tau=opt[\"hardneg_tau\"], hardneg_boost=opt[\"hardneg_boost\"],\n",
        "                    )\n",
        "\n",
        "                    cbs: List[TrainerCallback] = [ConsoleLogger(), EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "                    if cfg.use_hardneg:\n",
        "                        cbs.append(HardNegSampler(no_rel_id=0, tau=cfg.hardneg_tau, boost=cfg.hardneg_boost))\n",
        "                    if extra_callbacks: cbs.extend(extra_callbacks)\n",
        "\n",
        "                    trainer = train_re(\n",
        "                        train_csv=train_csv, dev_csv=dev_csv,\n",
        "                        label_list=label_list, cfg=cfg, save_best_to=best_dir,\n",
        "                        callbacks=cbs\n",
        "                    )\n",
        "\n",
        "                    metrics = trainer.evaluate()\n",
        "                    row[\"micro_f1\"] = metrics.get(\"micro f1 score\")\n",
        "                    row[\"auprc\"] = metrics.get(\"auprc\")\n",
        "                    row[\"accuracy\"] = metrics.get(\"accuracy\")\n",
        "                    state = getattr(trainer, \"state\", None)\n",
        "                    row[\"best_ckpt\"] = getattr(state, \"best_model_checkpoint\", None)\n",
        "\n",
        "                    # Optional: TTA (MC Dropout)\n",
        "                    if opt.get(\"use_tta\", False):\n",
        "                        trainer.model.train()\n",
        "                        preds = []\n",
        "                        with torch.no_grad():\n",
        "                            for _ in range(int(opt.get(\"tta_n\", 4))):\n",
        "                                out = trainer.predict(trainer.eval_dataset)\n",
        "                                preds.append(out.predictions)\n",
        "                        tta_logits = np.mean(preds, axis=0)\n",
        "                        labels = trainer.predict(trainer.eval_dataset).label_ids\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        no_rel = label_list.index(\"no_relation\")\n",
        "                        y_hat = tta_logits.argmax(-1)\n",
        "                        row[\"micro_f1_tta\"] = f1_score(labels, y_hat, average=\"micro\", labels=[i for i in range(len(label_list)) if i!=no_rel]) * 100.0\n",
        "\n",
        "                except Exception as e:\n",
        "                    row[\"error\"] = f\"{type(e).__name__}: {e}\"\n",
        "                finally:\n",
        "                    row[\"seconds\"] = round(time.time() - t0, 2)\n",
        "                    results.append(row)\n",
        "                    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Îäî ÎÇ®Í∏∞Í≥†, Îü¨Îãù Î°úÍ∑∏ Ìè¥ÎçîÎßå Ï†ïÎ¶¨ÌïòÍ≥† Ïã∂ÏúºÎ©¥ ÏïÑÎûò Ï§Ñ Ï£ºÏÑù Ï≤òÎ¶¨\n",
        "                    shutil.rmtree(out_dir, ignore_errors=True)\n",
        "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                print(f\"[{idx}/{total}] done: {row['run']} | microF1={row['micro_f1']} | err={row['error']}\")\n",
        "\n",
        "    df = pd.DataFrame(results).sort_values(by=[\"micro_f1\",\"auprc\",\"accuracy\"], ascending=False, na_position=\"last\")\n",
        "    df.to_csv(os.path.join(base_out, f\"param_grid_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "cioBmmgqFmJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run.py\n",
        "# from grid_plus.py import run_grid_plus\n",
        "# from parts_config import DEFAULT_LABEL_LIST\n",
        "\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/Colab Notebooks/upstage/dataset/train.csv\",\n",
        "DEV_CSV   = None  # ÏóÜÏúºÎ©¥ None\n",
        "\n",
        "MODELS = [\n",
        "    \"klue/roberta-base\",\n",
        "    # \"microsoft/deberta-v3-base\",\n",
        "    # \"klue/roberta-large\",\n",
        "]\n",
        "\n",
        "HP_SPACE = {\n",
        "    # Í∏∞Î≥∏ HP\n",
        "    \"lr\":              [2e-5, 3e-5],\n",
        "    \"epochs\":          [10],\n",
        "    \"train_bsz\":       [16, 32],\n",
        "    \"max_len\":         [256, 320],\n",
        "    \"scheduler\":       [\"cosine\"],\n",
        "    \"warmup_ratio\":    [0.05],\n",
        "    \"label_smoothing\": [0.0, 0.1],\n",
        "\n",
        "    # ÌëúÌòÑ\n",
        "    \"marker_variant\":  [\"typed\"],\n",
        "    \"inline_markers\":  [True],\n",
        "\n",
        "    # ÏÜêÏã§/Ï†ïÍ∑úÌôî\n",
        "    \"use_class_weight\":[False],\n",
        "    \"use_cb_loss\":     [True, False],\n",
        "    \"use_focal\":       [False, True],\n",
        "    \"focal_gamma\":     [2.0],\n",
        "    \"rdrop_alpha\":     [0.0, 2.0],\n",
        "\n",
        "    # Íµ¨Ï°∞/Ìä∏Î¶≠\n",
        "    \"use_marker_head\": [True, False],\n",
        "    \"use_erpe\":        [False, True],\n",
        "    \"erpe_dim\":        [32],\n",
        "    \"use_fgm\":         [False, True],\n",
        "    \"fgm_eps\":         [1e-3],\n",
        "\n",
        "    # ÏµúÏ†ÅÌôî\n",
        "    \"use_llrd\":        [False, True],\n",
        "    \"llrd_decay\":      [0.95],\n",
        "\n",
        "    # ÌïòÎìú ÎÑ§Í±∞Ìã∞Î∏å\n",
        "    \"use_hardneg\":     [False, True],\n",
        "    \"hardneg_tau\":     [0.55],\n",
        "    \"hardneg_boost\":   [2.0],\n",
        "\n",
        "    # ÌèâÍ∞Ä ÏòµÏÖò\n",
        "    \"use_tta\":         [False],   # ÌïÑÏöî Ïãú True Ï∂îÍ∞Ä\n",
        "    \"tta_n\":           [4],\n",
        "}\n",
        "\n",
        "\n",
        "df = run_grid_plus(\n",
        "      train_csv=TRAIN_CSV,\n",
        "      dev_csv=DEV_CSV,\n",
        "      models=MODELS,\n",
        "      hp_space=HP_SPACE,\n",
        "      base_out=\"/content/drive/MyDrive/Colab Notebooks/upstage/grid_runs_plus\",\n",
        "      seed_list=[42],\n",
        "      label_list=DEFAULT_LABEL_LIST,\n",
        ")\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/upstage/result.csv\")\n",
        "  # ÏÉÅÏúÑ 10Í∞úÎßå Ï∂úÎ†•\n",
        "with pd.option_context('display.max_columns', None):\n",
        "    print(df.head(10))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "kKSla2jhFoEp",
        "outputId": "899827b5-5f3e-4ee6-a347-19eda40688d8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/4096] done: klue_roberta-base_lr2e-05_ep10_bs16_ml256_mktyped_inline1_mh1_erpe0_ed32_fgm0_cb1_cw0_focal0_rd0.0_llrd0_hn0_tau0.55_boost2.0_seed42 | microF1=None | err=AttributeError: 'TrainConfig' object has no attribute 'es_patience'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2/4096] done: klue_roberta-base_lr2e-05_ep10_bs16_ml256_mktyped_inline1_mh1_erpe0_ed32_fgm0_cb1_cw0_focal0_rd0.0_llrd0_hn1_tau0.55_boost2.0_seed42 | microF1=None | err=AttributeError: 'TrainConfig' object has no attribute 'es_patience'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4236397704.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m df = run_grid_plus(\n\u001b[0m\u001b[1;32m     58\u001b[0m       \u001b[0mtrain_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_CSV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mdev_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEV_CSV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2107542429.py\u001b[0m in \u001b[0;36mrun_grid_plus\u001b[0;34m(train_csv, dev_csv, models, hp_space, base_out, seed_list, label_list, extra_callbacks)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mextra_callbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     trainer = train_re(\n\u001b[0m\u001b[1;32m    115\u001b[0m                         \u001b[0mtrain_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                         \u001b[0mlabel_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4003847987.py\u001b[0m in \u001b[0;36mtrain_re\u001b[0;34m(train_csv, dev_csv, label_list, cfg, save_best_to, callbacks)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0my_dv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdev_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tok_tr = tokenized_dataset(train_df, tokenizer,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                \u001b[0minline_markers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_markers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker_variant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarker_variant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                max_len=cfg.max_length, use_erpe=cfg.use_erpe)\n",
            "\u001b[0;32m/tmp/ipython-input-3183008656.py\u001b[0m in \u001b[0;36mtokenized_dataset\u001b[0;34m(df, tokenizer, inline_markers, marker_variant, use_unk, max_len, use_erpe)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minline_markers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             text = _inline_mark(\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subj_word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3183008656.py\u001b[0m in \u001b[0;36m_inline_mark\u001b[0;34m(sentence, s_word, o_word, s_type, o_type, use_type, use_unk)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0ms_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"[E1]{s_word}[/E1]\"\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mo_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"[E2]{o_word}[/E2]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepl_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepl_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3183008656.py\u001b[0m in \u001b[0;36mrepl_first\u001b[0;34m(text, pat, repl)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Ï≤´ Îì±Ïû•Îßå ÏπòÌôò (Îã®Ïñ¥ Í≤ΩÍ≥Ñ Í≥†Î†§)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrepl_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf'(?<!\\w){re.escape(pat)}(?!\\w)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    175\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    292\u001b[0m                   \u001b[0;34m\"Don't use it.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                   DeprecationWarning)\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/_compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0mgroupindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mindexgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroupindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         \u001b[0mindexgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDdlC-f_Nggc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}