{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1eZt9JCIO0pQQk6kNf98h0bMdz7vtgJAM",
      "authorship_tag": "ABX9TyMw1QGzb392V+m+SwZbJBEa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cheeyoung-Yoon/upstage_test/blob/main/transfomer_model_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "class RE_Dataset(torch.utils.data.Dataset):\n",
        "  \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
        "  def __init__(self, pair_dataset, labels):\n",
        "    self.pair_dataset = pair_dataset\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "def preprocessing_dataset(dataset):\n",
        "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
        "  subject_entity = []\n",
        "  object_entity = []\n",
        "  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
        "    i = i[1:-1].split(',')[0].split(':')[1]\n",
        "    j = j[1:-1].split(',')[0].split(':')[1]\n",
        "\n",
        "    subject_entity.append(i)\n",
        "    object_entity.append(j)\n",
        "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})\n",
        "  return out_dataset\n",
        "\n",
        "def load_data(dataset_dir):\n",
        "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
        "  pd_dataset = pd.read_csv(dataset_dir)\n",
        "  dataset = preprocessing_dataset(pd_dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def tokenized_dataset(dataset, tokenizer, use_type_markers=True, use_unk=True, max_len=256):\n",
        "    \"\"\"\n",
        "    dataset: pandas.DataFrame with columns:\n",
        "      - sentence\n",
        "      - subject_entity, object_entity  (dict-like str: {'word':..., 'type':...})\n",
        "    \"\"\"\n",
        "    import ast\n",
        "\n",
        "    def parse_ent(e):\n",
        "        if isinstance(e, str):\n",
        "            try:\n",
        "                e = ast.literal_eval(e)\n",
        "            except:\n",
        "                return None, None\n",
        "        if isinstance(e, dict):\n",
        "            return e.get(\"word\"), e.get(\"type\")\n",
        "        return None, None\n",
        "\n",
        "    enc_inputs, enc_texts = [], []\n",
        "\n",
        "    for s_ent, o_ent, sent in zip(dataset['subject_entity'], dataset['object_entity'], dataset['sentence']):\n",
        "        s_word, s_type = parse_ent(s_ent)\n",
        "        o_word, o_type = parse_ent(o_ent)\n",
        "\n",
        "        # 단어가 누락된 경우 안전장치\n",
        "        s_word = s_word if s_word else \"<SUBJ>\"\n",
        "        o_word = o_word if o_word else \"<OBJ>\"\n",
        "\n",
        "        if use_type_markers:\n",
        "            if not s_type and use_unk: s_type = \"UNK\"\n",
        "            if not o_type and use_unk: o_type = \"UNK\"\n",
        "\n",
        "            if s_type and o_type:\n",
        "                e_span = f\"[E1-{s_type}]{s_word}[/E1] [E2-{o_type}]{o_word}[/E2]\"\n",
        "            else:\n",
        "                # 타입을 전혀 모르면 타입 없는 일반 마커 사용\n",
        "                e_span = f\"[E1]{s_word}[/E1] [E2]{o_word}[/E2]\"\n",
        "        else:\n",
        "            # 타입 마커 비활성화: 일반 마커만\n",
        "            e_span = f\"[E1]{s_word}[/E1] [E2]{o_word}[/E2]\"\n",
        "\n",
        "        enc_inputs.append(e_span)\n",
        "        enc_texts.append(sent)\n",
        "\n",
        "    # 필요 시 특수 토큰 등록 (한 번만 실행)\n",
        "    # 타입 마커/일반 마커/종료 마커 + UNK\n",
        "    special_tokens = {\"additional_special_tokens\": [\n",
        "        \"[E1]\",\"[/E1]\",\"[E2]\",\"[/E2]\",\n",
        "        \"[E1-PER]\",\"[E2-PER]\",\"[E1-ORG]\",\"[E2-ORG]\",\n",
        "        \"[E1-LOC]\",\"[E2-LOC]\",\"[E1-UNK]\",\"[E2-UNK]\"\n",
        "    ]}\n",
        "    num_added = tokenizer.add_special_tokens(special_tokens)\n",
        "    # model.resize_token_embeddings(len(tokenizer))  # 모델 로드 후 1회 실행\n",
        "\n",
        "    return tokenizer(\n",
        "        enc_inputs,\n",
        "        enc_texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "RnYnHIdtfL9a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle, numpy as np, pandas as pd, torch, sklearn\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Any\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# ===== Metrics (model-agnostic, dynamic num_labels) =====\n",
        "def micro_f1_wo_no_relation(preds, labels, label_list: List[str], no_rel: str = \"no_relation\"):\n",
        "    no_rel_idx = label_list.index(no_rel)\n",
        "    use_labels = list(range(len(label_list)))\n",
        "    use_labels.remove(no_rel_idx)\n",
        "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=use_labels) * 100.0\n",
        "\n",
        "def auprc_all(probs, labels, num_labels: int):\n",
        "    labels_oh = np.eye(num_labels)[labels]\n",
        "    score = np.zeros((num_labels,), dtype=np.float32)\n",
        "    for c in range(num_labels):\n",
        "        targets_c = labels_oh[:, c]\n",
        "        preds_c = probs[:, c]\n",
        "        p, r, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
        "        score[c] = sklearn.metrics.auc(r, p)\n",
        "    return float(np.mean(score) * 100.0)\n",
        "\n",
        "def make_compute_metrics(label_list: List[str], no_rel: str = \"no_relation\"):\n",
        "    num_labels = len(label_list)\n",
        "    def _compute(eval_pred):\n",
        "        logits = eval_pred.predictions\n",
        "        probs  = logits if logits.ndim == 2 else logits[0]\n",
        "        preds  = probs.argmax(-1)\n",
        "        labels = eval_pred.label_ids\n",
        "        return {\n",
        "            \"micro f1 score\": micro_f1_wo_no_relation(preds, labels, label_list, no_rel),\n",
        "            \"auprc\": auprc_all(probs, labels, num_labels),\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "        }\n",
        "    return _compute\n",
        "\n",
        "# ===== Config =====\n",
        "DEFAULT_LABEL_LIST = [\n",
        "    'no_relation', 'org:top_members/employees', 'org:members', 'org:product', 'per:title',\n",
        "    'org:alternate_names', 'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
        "    'org:number_of_employees/members', 'per:children', 'per:place_of_residence',\n",
        "    'per:alternate_names', 'per:other_family', 'per:colleagues', 'per:origin',\n",
        "    'per:siblings', 'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
        "    'org:member_of', 'per:parents', 'org:dissolved', 'per:schools_attended',\n",
        "    'per:date_of_death', 'per:date_of_birth', 'per:place_of_birth', 'per:place_of_death',\n",
        "    'org:founded_by', 'per:religion'\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    model_name: str = \"klue/bert-base\"          # BERT / RoBERTa / ELECTRA 모두 OK\n",
        "    output_dir: str = \"./results\"\n",
        "    num_train_epochs: int = 10\n",
        "    learning_rate: float = 5e-5\n",
        "    per_device_train_batch_size: int = 16\n",
        "    per_device_eval_batch_size: int = 16\n",
        "    warmup_steps: int = 500\n",
        "    weight_decay: float = 0.01\n",
        "    logging_steps: int = 100\n",
        "    save_steps: int = 500\n",
        "    eval_steps: int = 500\n",
        "    save_total_limit: int = 5\n",
        "    load_best_model_at_end: bool = True\n",
        "    seed: int = 42\n",
        "    max_length: int = 256\n",
        "    fp16: bool = False                         # True로 주면 A100/3090 등에서 mixed precision\n",
        "    special_tokens: Optional[List[str]] = None # 예: [\"[E1]\",\"[/E1]\",\"[E2]\",\"[/E2]\"]\n",
        "\n",
        "\n",
        "# ===== Main train function =====\n",
        "def train_re(\n",
        "    train_csv: str,\n",
        "    dev_csv: Optional[str] = None,\n",
        "    label_list: List[str] = DEFAULT_LABEL_LIST,\n",
        "    cfg: TrainConfig = TrainConfig(),\n",
        "    label_map_path: str = 'dict_label_to_num.pkl',\n",
        "    save_best_to: str = \"./best_model\",\n",
        "):\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    np.random.seed(cfg.seed)\n",
        "\n",
        "    # ---- 1) Tokenizer / Model (순서 중요) ----\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "\n",
        "    # [FIX] 마커 특수토큰을 먼저 추가\n",
        "    added = 0\n",
        "    if cfg.special_tokens:\n",
        "        added = tokenizer.add_special_tokens({\"additional_special_tokens\": cfg.special_tokens})\n",
        "        if added > 0:\n",
        "            print(f\"[info] added {added} special tokens\")\n",
        "\n",
        "    num_labels = len(label_list)\n",
        "    model_config = AutoConfig.from_pretrained(cfg.model_name, num_labels=num_labels)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(cfg.model_name, config=model_config)\n",
        "\n",
        "    # [FIX] 특수토큰 추가했으면 반드시 임베딩 리사이즈\n",
        "    if added > 0:\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # ---- 2) Load data & label mapping ----\n",
        "    train_df = load_data(train_csv)\n",
        "    label_map = {label: idx for idx, label in enumerate(label_list)}\n",
        "\n",
        "    # [FIX] 라벨이 리스트 밖이면 KeyError 방지\n",
        "    try:\n",
        "        train_y = [label_map[v] for v in train_df['label'].values]\n",
        "    except KeyError as e:\n",
        "        missing = set(train_df['label'].unique()) - set(label_list)\n",
        "        raise ValueError(f\"Found labels not in label_list: {missing}\") from e\n",
        "\n",
        "    # ---- 3) Tokenize (최종 tokenizer로!) ----\n",
        "    tokenized_train = tokenized_dataset(train_df, tokenizer)\n",
        "    # [FIX] RoBERTa 호환: token_type_ids 제거(있으면)\n",
        "    if isinstance(tokenized_train, dict):\n",
        "        tokenized_train.pop(\"token_type_ids\", None)\n",
        "\n",
        "    tokenized_train.pop(\"token_type_ids\", None)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb = model.get_input_embeddings()\n",
        "        vocab_size = emb.weight.size(0)\n",
        "        max_id = int(tokenized_train[\"input_ids\"].max().item())\n",
        "        print(f\"[check] vocab_size={vocab_size}, max_input_id={max_id}\")\n",
        "\n",
        "        if max_id >= vocab_size:\n",
        "            # 디버그: 어떤 토큰들이 범위를 넘는지 확인\n",
        "            ids = tokenized_train[\"input_ids\"].view(-1)\n",
        "            bad_ids = ids[ids >= vocab_size].unique().tolist()\n",
        "            bad_toks = [tokenizer.convert_ids_to_tokens(int(i)) for i in bad_ids]\n",
        "            print(f\"[warn] out-of-vocab ids: {bad_ids}\")\n",
        "            print(f\"[warn] out-of-vocab tokens: {bad_toks}\")\n",
        "\n",
        "            # 1) 가장 보수적인 즉시 복구: 임베딩을 입력의 최대 id+1 로 리사이즈\n",
        "            new_size = max_id + 1\n",
        "            print(f\"[fix] resize embeddings to {new_size}\")\n",
        "            model.resize_token_embeddings(new_size)\n",
        "            vocab_size = new_size  # 갱신\n",
        "    RE_train = RE_Dataset(tokenized_train, train_y)\n",
        "\n",
        "    if dev_csv is not None:\n",
        "        dev_df = load_data(dev_csv)\n",
        "        try:\n",
        "            dev_y = [label_map[v] for v in dev_df['label'].values]\n",
        "        except KeyError as e:\n",
        "            missing = set(dev_df['label'].unique()) - set(label_list)\n",
        "            raise ValueError(f\"[dev] labels not in label_list: {missing}\") from e\n",
        "\n",
        "        tokenized_dev = tokenized_dataset(dev_df, tokenizer)\n",
        "        tokenized_dev.pop(\"token_type_ids\", None)\n",
        "        # dev에서도 안전검사(선택)\n",
        "        with torch.no_grad():\n",
        "            max_id_dev = int(tokenized_dev[\"input_ids\"].max().item())\n",
        "            if max_id_dev >= vocab_size:\n",
        "                raise RuntimeError(\n",
        "                    f\"[dev] Input id ({max_id_dev}) >= embedding size ({vocab_size}). \"\n",
        "                    f\"Did tokenizer change after tokenizing?\"\n",
        "                )\n",
        "        RE_dev = RE_Dataset(tokenized_dev, dev_y)\n",
        "    else:\n",
        "        RE_dev = None\n",
        "\n",
        "    # ---- 4) TrainingArguments (HF 4.55 API: eval_strategy 사용) ----\n",
        "    has_dev = RE_dev is not None\n",
        "    evaluation_strategy = 'steps' if RE_dev is not None else 'no'\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=cfg.output_dir,\n",
        "        save_total_limit=cfg.save_total_limit,\n",
        "        save_steps=cfg.save_steps,\n",
        "        num_train_epochs=cfg.num_train_epochs,\n",
        "        learning_rate=cfg.learning_rate,\n",
        "        per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=cfg.per_device_eval_batch_size,\n",
        "        warmup_steps=cfg.warmup_steps,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "        logging_dir=os.path.join(cfg.output_dir, \"logs\"),\n",
        "        logging_steps=cfg.logging_steps,\n",
        "        eval_strategy=evaluation_strategy,            # ← 이름 주의\n",
        "        eval_steps=cfg.eval_steps if has_dev else None,\n",
        "        load_best_model_at_end=cfg.load_best_model_at_end if has_dev else False,\n",
        "        metric_for_best_model=\"micro f1 score\" if has_dev else None,  # ← EarlyStopping용\n",
        "        greater_is_better=True,\n",
        "        seed=cfg.seed,\n",
        "        fp16=cfg.fp16,\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_pin_memory=torch.cuda.is_available(),\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # === 5) Trainer ===\n",
        "    compute_metrics = make_compute_metrics(label_list, no_rel=\"no_relation\") if has_dev else None\n",
        "\n",
        "    callbacks = []\n",
        "    if has_dev and training_args.metric_for_best_model and training_args.load_best_model_at_end:\n",
        "        callbacks.append(EarlyStoppingCallback(\n",
        "            early_stopping_patience=2,\n",
        "            early_stopping_threshold=0.002,\n",
        "        ))\n",
        "    # dev 없으면 EarlyStopping 미사용\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=RE_train,\n",
        "        eval_dataset=RE_dev,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    # ---- 6) Train ----\n",
        "    trainer.train()\n",
        "\n",
        "    # ---- 7) Save best (or final) model ----\n",
        "    os.makedirs(save_best_to, exist_ok=True)\n",
        "    trainer.save_model(save_best_to)\n",
        "    if trainer.tokenizer is not None:\n",
        "        trainer.tokenizer.save_pretrained(save_best_to)\n",
        "\n",
        "    print(f\"Model saved to: {save_best_to}\")\n",
        "    return trainer\n",
        "\n"
      ],
      "metadata": {
        "id": "U09rUtxKe91h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grid_runner.py\n",
        "import os, gc, traceback, json, time\n",
        "import torch\n",
        "import pandas as pd\n",
        "from dataclasses import replace\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# 당신이 제공한 train_re, TrainConfig, DEFAULT_LABEL_LIST 를 import\n",
        "# from train_module import train_re, TrainConfig, DEFAULT_LABEL_LIST\n",
        "\n",
        "def run_grid(\n",
        "    train_csv: str,\n",
        "    dev_csv: Optional[str],\n",
        "    models: List[str],\n",
        "    lrs: List[float] = (5e-5, 3e-5, 2e-5),\n",
        "    epochs: List[int] = (5, 10),\n",
        "    train_bsz: List[int] = (16,),\n",
        "    eval_bsz: List[int] = (16,),\n",
        "    seed: int = 42,\n",
        "    base_out: str = \"./grid_runs\",\n",
        "    label_list: List[str] = None,\n",
        "    use_fp16_if_cuda: bool = True,\n",
        "    special_tokens: Optional[List[str]] = [\"[E1]\",\"[/E1]\",\"[E2]\",\"[/E2]\",\n",
        "                                           \"[E1-PER]\",\"[E2-PER]\",\"[E1-ORG]\",\"[E2-ORG]\",\n",
        "                                           \"[E1-LOC]\",\"[E2-LOC]\",\"[E1-UNK]\",\"[E2-UNK]\"],\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    여러 모델/하이퍼파라미터 조합을 순차적으로 학습하고 dev 성능을 수집하여 DataFrame으로 반환.\n",
        "    실패한 러나는 error 컬럼에 스택트레이스를 남김.\n",
        "    \"\"\"\n",
        "    if label_list is None:\n",
        "        from __main__ import DEFAULT_LABEL_LIST as _DEF  # 노트북에서 직접 실행 대비\n",
        "        label_list = _DEF\n",
        "\n",
        "    os.makedirs(base_out, exist_ok=True)\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    combo_idx = 0\n",
        "    total = len(models) * len(lrs) * len(epochs) * len(train_bsz)\n",
        "    print(f\"[grid] total runs: {total}\")\n",
        "\n",
        "    for model_name in models:\n",
        "        for lr in lrs:\n",
        "            for ep in epochs:\n",
        "                for bsz in train_bsz:\n",
        "                    combo_idx += 1\n",
        "                    run_name = f\"{model_name.replace('/','_')}_lr{lr:g}_ep{ep}_bs{bsz}\"\n",
        "                    out_dir = os.path.join(base_out, run_name)\n",
        "                    best_dir = os.path.join(out_dir, \"best\")\n",
        "\n",
        "                    print(f\"\\n[grid {combo_idx}/{total}] {run_name}\")\n",
        "\n",
        "                    # CUDA/메모리 정리\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                    cfg = TrainConfig(\n",
        "                        model_name=model_name,\n",
        "                        output_dir=out_dir,\n",
        "                        num_train_epochs=ep,\n",
        "                        learning_rate=lr,\n",
        "                        per_device_train_batch_size=bsz,\n",
        "                        per_device_eval_batch_size=eval_bsz[0],\n",
        "                        warmup_steps=0,\n",
        "                        weight_decay=0.01,\n",
        "                        logging_steps=50,\n",
        "                        save_steps=200,\n",
        "                        eval_steps=200 if dev_csv else None,\n",
        "                        save_total_limit=3,\n",
        "                        load_best_model_at_end=bool(dev_csv),\n",
        "                        seed=seed,\n",
        "                        max_length=256,\n",
        "                        fp16=torch.cuda.is_available() and use_fp16_if_cuda,\n",
        "                        special_tokens=special_tokens,\n",
        "                    )\n",
        "\n",
        "                    row: Dict[str, Any] = {\n",
        "                        \"model\": model_name, \"lr\": lr, \"epochs\": ep, \"train_bsz\": bsz,\n",
        "                        \"output_dir\": out_dir, \"best_dir\": best_dir,\n",
        "                        \"micro_f1\": None, \"auprc\": None, \"accuracy\": None,\n",
        "                        \"best_ckpt\": None, \"error\": None, \"seconds\": None,\n",
        "                    }\n",
        "\n",
        "                    t0 = time.time()\n",
        "                    try:\n",
        "                        trainer = train_re(\n",
        "                            train_csv=train_csv,\n",
        "                            dev_csv=dev_csv,\n",
        "                            label_list=label_list,\n",
        "                            cfg=cfg,\n",
        "                            save_best_to=best_dir,\n",
        "                        )\n",
        "\n",
        "                        # dev가 있으면 evaluate로 표준화된 측정\n",
        "                        if dev_csv:\n",
        "                            metrics = trainer.evaluate()\n",
        "                            # 키가 \"micro f1 score\"로 들어오므로 공백 제거한 alias도 만들어 둠\n",
        "                            row[\"micro_f1\"] = metrics.get(\"micro f1 score\")\n",
        "                            row[\"auprc\"]    = metrics.get(\"auprc\")\n",
        "                            row[\"accuracy\"] = metrics.get(\"accuracy\")\n",
        "                        else:\n",
        "                            # dev 없을 경우 마지막 train logs에서 꺼내거나 None\n",
        "                            pass\n",
        "\n",
        "                        # best checkpoint 경로\n",
        "                        state = getattr(trainer, \"state\", None)\n",
        "                        row[\"best_ckpt\"] = getattr(state, \"best_model_checkpoint\", None)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        row[\"error\"] = f\"{type(e).__name__}: {e}\\n\" + traceback.format_exc(limit=2)\n",
        "                        print(\"[error]\", row[\"error\"])\n",
        "                    finally:\n",
        "                        row[\"seconds\"] = round(time.time() - t0, 2)\n",
        "                        results.append(row)\n",
        "\n",
        "                        # GPU 메모리 정리\n",
        "                        if torch.cuda.is_available():\n",
        "                            torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "    df = pd.DataFrame(results).sort_values(\n",
        "        by=[\"micro_f1\", \"auprc\", \"accuracy\"], ascending=False, na_position=\"last\"\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # CSV로도 저장\n",
        "    csv_path = os.path.join(base_out, \"grid_summary.csv\")\n",
        "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\n[grid] summary saved: {csv_path}\")\n",
        "    return df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jbAyj4DqgEbp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS = [\n",
        "    \"klue/bert-base\",\n",
        "    \"klue/roberta-base\",\n",
        "    \"monologg/koelectra-base-v3-discriminator\",\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    \"kykim/bert-kor-base\",\n",
        "    \"monologg/koelectra-base-v3\",\n",
        "    \"BM-K/KoSimCSE-roberta-multitask\"\n",
        "    # 필요시 추가\n",
        "]\n",
        "\n",
        "df = run_grid(\n",
        "    train_csv=\"/content/drive/MyDrive/Colab Notebooks/upstage/dataset/train.csv\",\n",
        "    dev_csv=None,     # dev가 없으면 None\n",
        "    models=MODELS,\n",
        "    lrs=[5e-5, 3e-5, 2e-5],\n",
        "    epochs=[5, 10],\n",
        "    train_bsz=[16, 32],\n",
        "    base_out=\"./grid_runs_re\",\n",
        ")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "MioJxwHRONvt",
        "outputId": "07eea89e-286b-4ff3-910b-a56a0aa90437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[grid] total runs: 84\n",
            "\n",
            "[grid 1/84] klue_bert-base_lr5e-05_ep5_bs16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] added 12 special tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[check] vocab_size=32012, max_input_id=32011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-415680640.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='258' max='10150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  258/10150 00:16 < 10:29, 15.72 it/s, Epoch 0.13/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.547500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.247400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.020600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.779300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smTo_pkaOZIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}